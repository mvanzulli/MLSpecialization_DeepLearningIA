import numpy as np
import matplotlib.pyplot as plt
from matplotlib.gridspec import GridSpec
plt.style.use('./deeplearning.mplstyle')
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LeakyReLU
from tensorflow.keras.activations import linear, relu, sigmoid



## 1 - ReLU Activation (Most used in hidden layers in logistic regression problemsf )
# A new activation was introduced, the Rectified Linear Unit (ReLU). 
# $$ a = max(0,z) \quad\quad\text {# ReLU function} $$
# The function shown is composed of linear pieces (piecewise linear).
#  The slope is consistent during the linear portion and then changes abruptly 
# at transition points. At transition points, a new linear function is added which,
#  when added to the existing function, will produce the new slope. 
# The new function is added at transition point but does not contribute to the 
# output prior to that point. The non-linear activation function is responsible for disabling
#  the input prior to and sometimes after the transition points. The following exercise provides
#  a more tangible example.


## 2 - SoftMax Activation 
# In both softmax regression and neural networks with Softmax outputs, N outputs are
#  generated and one output is selected as the predicted category. In both cases a vector
#  𝐳  is generated by a linear function which is applied to a softmax function. 
# The softmax function converts  𝐳  into a probability distribution as described below. 
# After applying softmax, each output will be between 0 and 1 and the outputs will add
#  to 1, so that they can be interpreted as probabilities. The larger inputs will correspond to larger output probabilities.

#
# The softmax function can be written:
# $$a_j = \frac{e^{z_j}}{ \sum_{k=1}^{N}{e^{z_k} }} \tag{1}$$
# The output $\mathbf{a}$ is a vector of length N, so for softmax regression, you could also write:
# \begin{align}
# \mathbf{a}(x) =
# \begin{bmatrix}
# P(y = 1 | \mathbf{x}; \mathbf{w},b) \\
# \vdots \\
# P(y = N | \mathbf{x}; \mathbf{w},b)
# \end{bmatrix}
# =
# \frac{1}{ \sum_{k=1}^{N}{e^{z_k} }}
# \begin{bmatrix}
# e^{z_1} \\
# \vdots \\
# e^{z_{N}} \\
# \end{bmatrix} \tag{2}
# \end{align}

# Which shows the output is a vector of probabilities. The first entry is the probability 
# the input is the first category given the input $\mathbf{x}$ and parameters $\mathbf{w}$ 
# and $\mathbf{b}$.  
# Let's create a NumPy implementation:

def my_softmax(z):
    ez = np.exp(z)              #element-wise exponenial
    sm = ez/np.sum(ez)
    return(sm)


# the exponential in the numerator of the softmax magnifies small differences in the values 
# * the output values sum to one
# * the softmax spans all of the outputs. A change in `z0` for example will change the values of `a0`-`a3`.
#  Compare this to other activations such as ReLU or Sigmoid which have a single input and single output.


## Cost function 

# The loss function associated with Softmax, the cross-entropy loss, is:
# \begin{equation}
#   L(\mathbf{a},y)=\begin{cases}
#     -log(a_1), & \text{if $y=1$}.\\
#         &\vdots\\
#      -log(a_N), & \text{if $y=N$}
#   \end{cases} \tag{3}
# \end{equation}

# Where y is the target category for this example and $\mathbf{a}$ is the output of a softmax function. In particular, the values in $\mathbf{a}$ are probabilities that sum to one.
# >**Recall:** In this course, Loss is for one example while Cost covers all examples. 
 
 
# Note in (3) above, only the line that corresponds to the target contributes to the loss, other lines are zero. To write the cost equation we need an 'indicator function' that will be 1 when the index matches the target and zero otherwise. 
#     $$\mathbf{1}\{y == n\} = =\begin{cases}
#     1, & \text{if $y==n$}.\\
#     0, & \text{otherwise}.
#   \end{cases}$$
# Now the cost is:
# \begin{align}
# J(\mathbf{w},b) = -\frac{1}{m} \left[ \sum_{i=1}^{m} \sum_{j=1}^{N}  1\left\{y^{(i)} == j\right\} \log \frac{e^{z^{(i)}_j}}{\sum_{k=1}^N e^{z^{(i)}_k} }\right] \tag{4}
# \end{align}

# Where $m$ is the number of examples, $N$ is the number of outputs. This is the average of all the losses.


## Tensor flow implementation

# The model below is implemented with the softmax as an activation in the final Dense layer.
# The loss function is separately specified in the `compile` directive. 

# The loss function is `SparseCategoricalCrossentropy`. This loss is described in (3) above.
#  In this model, the softmax takes place in the last layer. 
# The loss function takes in the softmax output which is a vector of probabilities. 

model = Sequential(
    [ 
        Dense(25, activation = 'relu'),
        Dense(15, activation = 'relu'),
        Dense(4, activation = 'softmax')    # < softmax activation here
    ]
)
model.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(0.001),
)

model.fit(
    X_train,y_train,
    epochs=10
)


# Recall from lecture, more stable and accurate results can be obtained if the softmax and loss 
# are combined during training.  
#  This is enabled by the 'preferred' organization shown here.

# In the preferred organization the final layer has a linear activation. 
# For historical reasons, the outputs in this form are referred to as *logits*. 
# The loss function has an additional argument: `from_logits = True`. 
# This informs the loss function that the softmax operation should be included in the loss calculation. 
# This allows for an optimized implementation.

preferred_model = Sequential(
    [ 
        Dense(25, activation = 'relu'),
        Dense(15, activation = 'relu'),
        Dense(4, activation = 'linear')   #<-- Note
    ]
)
preferred_model.compile(
    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #<-- Note
    optimizer=tf.keras.optimizers.Adam(0.001),
)

preferred_model.fit(
    X_train,y_train,
    epochs=10
)

#### Output Handling
# Notice that in the preferred model, the outputs are not probabilities, 
# but can range from large negative numbers to large positive numbers. 
# The output must be sent through a softmax when performing a prediction that expects a probability. 
# Let's look at the preferred model outputs:

p_preferred = preferred_model.predict(X_train)
print(f"two example output vectors:\n {p_preferred[:2]}")
print("largest value", np.max(p_preferred), "smallest value", np.min(p_preferred))

# The output predictions are not probabilities!
# If the desired output are probabilities, the output should be be processed by a 
# [softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax).

## SparseCategorialCrossentropy or CategoricalCrossEntropy
# Tensorflow has two potential formats for target values and the selection of the loss defines which is expected.
# - SparseCategorialCrossentropy: expects the target to be an integer corresponding to the index.
#  For example, if there are 10 potential target values, y would be between 0 and 9. 
# - CategoricalCrossEntropy: Expects the target value of an example to be one-hot encoded where the value
#  at the target index is 1 while the other N-1 entries are zero. An example with 10 potential target values, 
# where the target is 2 would be [0,0,1,0,0,0,0,0,0,0].
