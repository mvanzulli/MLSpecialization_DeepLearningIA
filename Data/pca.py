# PCA - An example on Exploratory Data Analysis

# In this notebook you will:

# - Replicate Andrew's example on PCA
# - Visualize how PCA works on a 2-dimensional small dataset and that not every projection is "good"
# - Visualize how a 3-dimensional data can also be contained in a 2-dimensional subspace
# - Use PCA to find hidden patterns in a high-dimensional dataset


import pandas as pd
import numpy as np
from sklearn.decomposition import PCA
from pca_utils import plot_widget
from bokeh.io import show, output_notebook
from bokeh.plotting import figure
import matplotlib.pyplot as plt
import plotly.offline as py
import plotly.express as px

# py.init_notebook_mode()

# output_notebook()

# Toy example

X = np.array([[ 99,  -1],
       [ 98,  -1],
       [ 97,  -2],
       [101,   1],
       [102,   1],
       [103,   2]])
plt.cla()
plt.plot(X[:,0], X[:,1], 'ro')
plt.show()

# Loding PCA algo
pca_2 = PCA(n_components=2)
print("PCA with 2 components is:", pca_2)

# Let's fit the data. We do not need to scale it, since sklearn's implementation already handles it.
pca_2.fit(X)

# The coordinates on the first principal component (first axis) are enough to retain 99.24% of the information ("explained variance"). 
# The second principal component adds an additional 0.76% of the information ("explained variance") that is not stored in the first principal component coordinates.

print("PCA axis ratio", pca_2.explained_variance_ratio_)

X_trans_2 = pca_2.transform(X)
print(X_trans_2)

# Inverse transform
X_reduced_2 = pca_2.inverse_transform(X_trans_2)
print("The inverse transform (2 axis) of X is:",X_reduced_2)
plt.cla()
plt.plot(X_reduced_2[:,0], X_reduced_2[:,1], 'ro')
plt.show()

# Think of column 1 as the coordinate along the first principal component (the first new axis) and column 2
#  as the coordinate along the second principal component (the second new axis).

# Can probably just choose the first principal component since it retains 99% of the information (explained variance)

# Loding PCA algo
pca_1 = PCA(n_components=1)
print("PCA with 1 components is:", pca_1)

# Let's fit the data. We do not need to scale it, since sklearn's implementation already handles it.
pca_1.fit(X)

# The coordinates on the first principal component (first axis) are enough to retain 99.24% of the information ("explained variance"). 
# The second principal component adds an additional 0.76% of the information ("explained variance") that is not stored in the first principal component coordinates.

print("PCA axis ratio", pca_1.explained_variance_ratio_)

X_trans_1 = pca_1.transform(X)
print(X_trans_1)

# Inverse transform
X_reduced_1 = pca_1.inverse_transform(X_trans_1)
X_reduced_1
print("The inverse transform of X (1 axis) is:",X_reduced_1)

plt.cla()
plt.plot(X_reduced_1[:,0], X_reduced_1[:,1], 'ro')
plt.show()



## Visualizing the PCA algorithm 
# Let's define $10$ points in the plane and use them as an example to visualize how we can compress this points in 1 dimension.
#  You will see that there are good ways and bad ways.

X = np.array([[-0.83934975, -0.21160323],
       [ 0.67508491,  0.25113527],
       [-0.05495253,  0.36339613],
       [-0.57524042,  0.24450324],
       [ 0.58468572,  0.95337657],
       [ 0.5663363 ,  0.07555096],
       [-0.50228538, -0.65749982],
       [-0.14075593,  0.02713815],
       [ 0.2587186 , -0.26890678],
       [ 0.02775847, -0.77709049]])


p = figure(title = '10-point scatterplot', x_axis_label = 'x-axis', y_axis_label = 'y-axis') ## Creates the figure object
p.scatter(X[:,0],X[:,1],marker = 'o', color = '#C00000', size = 5) ## Add the scatter plot

## Some visual adjustments
p.grid.visible = False
p.grid.visible = False
p.outline_line_color = None 
p.toolbar.logo = None
p.toolbar_location = None
p.xaxis.axis_line_color = "#f0f0f0"
p.xaxis.axis_line_width = 5
p.yaxis.axis_line_color = "#f0f0f0"
p.yaxis.axis_line_width = 5

## Shows the figure
# show(p)


## Plot a widget 
# The next code will generate a widget where you can see how different ways of compressing this data into 
# 1-dimensional datapoints will lead to different ways on how the points are spread in this new space. 
# The line generated by PCA is the line that keeps the points as far as possible from each other. 

# You can use the slider to rotate the black line through its center and see how the points' projection
#  onto the line will change as we rotate the line. 

# You can notice that there are projections that place different points in almost the same point, 
# and there are projections that keep the points as separated as they were in the plane.


## Using PCA in Exploratory Data Analysis

# Let's load a toy dataset with $500$ samples and $1000$ features.

df = pd.read_csv("toy_dataset.csv")


# This is a dataset with $1000$ features.

# Let's try to see if there is a pattern in the data.
#  The following function will randomly sample 100 pairwise tuples (x,y) of features, so we can scatter-plot them.

def get_pairs(n = 100):
    from random import randint
    i = 0
    tuples = []
    while i < n:
        x = df.columns[randint(0,999)]
        y = df.columns[randint(0,999)]
        while x == y and (x,y) in tuples or (y,x) in tuples:
            y = df.columns[randint(0,999)]
        tuples.append((x,y))
        i+=1
    return tuples

pairs = get_pairs()

# Plot 35 pairs of feateures

fig, axs = plt.subplots(10,10, figsize = (35,35))
i = 0
for rows in axs:
    for ax in rows:
        ax.scatter(df[pairs[i][0]],df[pairs[i][1]], color = "#C00000")
        ax.set_xlabel(pairs[i][0])
        ax.set_ylabel(pairs[i][1])
        i+=1


plt.show()

# Not much information can be divised with the hole dataset, we might use PCA 

# This may take 1 minute to run
corr = df.corr()

## This will show all the features that have correlation > 0.5 in absolute value. We remove the features 
## with correlation == 1 to remove the correlation of a feature with itself

mask = (abs(corr) > 0.5) & (abs(corr) != 1)
corr.where(mask).stack().sort_values()


# Loading the PCA object and transform it into 2 axis
pca = PCA(n_components = 2) # Here we choose the number of components that we will keep.
X_pca = pca.fit_transform(df)
df_pca = pd.DataFrame(X_pca, columns = ['principal_component_1','principal_component_2'])
print(df_pca.head())

plt.scatter(df_pca['principal_component_1'],df_pca['principal_component_2'], color = "#C00000")
plt.xlabel('principal_component_1')
plt.ylabel('principal_component_2')
plt.title('PCA decomposition')
plt.show()

print("The amount of variance explained by each principal component:", pca.explained_variance_ratio_)


# Loading the PCA object and transform it into 3 PCA axis
pca_3 = PCA(n_components = 3).fit(df)
X_t = pca_3.transform(df)
df_pca_3 = pd.DataFrame(X_t,columns = ['principal_component_1','principal_component_2','principal_component_3'])

fig = px.scatter_3d(df_pca_3, x = 'principal_component_1', y = 'principal_component_2', z = 'principal_component_3').update_traces(marker = dict(color = "#C00000"))
fig.show()

print("The amount of variance explained by each principal component:", pca_3.explained_variance_ratio_)